{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project Title: GAN with PyTorch Overview Briefly describe your project here. Explain that it's a Generative Adversarial Network implemented using PyTorch, focusing on generating images with the MNIST dataset. Example: This project implements a Generative Adversarial Network (GAN) using PyTorch to generate digit images similar to those in the MNIST dataset. It includes a Generator for creating images and a Discriminator for distinguishing between real and generated images. Installation Instructions for setting up the project environment. 1. Clone the repository: ``` git clone https://github.com/atikul-islam-sajib/GPSG.git ``` 2. Navigate to the project directory: ``` cd GPSG ``` 3. Install required dependencies (list any necessary libraries or frameworks): ``` pip install -r requirements.txt ``` Usage Provide a quick-start guide or examples of how to use your project. To train the GAN model: 1. Run the main script: ``` !python /content/GPSG/clf.py --batch_size 64 --lr 0.0002 --epochs 1 --latent_space 100 --download_mnist ``` 2. Check the generated images in the specified output directory. ``` !python /content/GPSG/clf.py --num_samplees 64 --generate ``` Features Highlight key features of your project, such as: GAN architecture with customizable layers. MNIST dataset integration for training. GPU support for efficient training. License Include information about your project's license. This project is licensed under the MIT License - see the LICENSE file for details.","title":"Home"},{"location":"#project-title-gan-with-pytorch","text":"","title":"Project Title: GAN with PyTorch"},{"location":"#overview","text":"Briefly describe your project here. Explain that it's a Generative Adversarial Network implemented using PyTorch, focusing on generating images with the MNIST dataset. Example: This project implements a Generative Adversarial Network (GAN) using PyTorch to generate digit images similar to those in the MNIST dataset. It includes a Generator for creating images and a Discriminator for distinguishing between real and generated images.","title":"Overview"},{"location":"#installation","text":"Instructions for setting up the project environment. 1. Clone the repository: ``` git clone https://github.com/atikul-islam-sajib/GPSG.git ``` 2. Navigate to the project directory: ``` cd GPSG ``` 3. Install required dependencies (list any necessary libraries or frameworks): ``` pip install -r requirements.txt ```","title":"Installation"},{"location":"#usage","text":"Provide a quick-start guide or examples of how to use your project. To train the GAN model: 1. Run the main script: ``` !python /content/GPSG/clf.py --batch_size 64 --lr 0.0002 --epochs 1 --latent_space 100 --download_mnist ``` 2. Check the generated images in the specified output directory. ``` !python /content/GPSG/clf.py --num_samplees 64 --generate ```","title":"Usage"},{"location":"#features","text":"Highlight key features of your project, such as: GAN architecture with customizable layers. MNIST dataset integration for training. GPU support for efficient training.","title":"Features"},{"location":"#license","text":"Include information about your project's license. This project is licensed under the MIT License - see the LICENSE file for details.","title":"License"},{"location":"data_loader/","text":"Loader This script facilitates the downloading and loading of the MNIST dataset for machine learning tasks. The script uses argparse to accept command-line arguments for batch size and a flag to trigger the download of the MNIST dataset. It defines a Loader class responsible for downloading the MNIST dataset and creating a DataLoader object from it. The DataLoader object is then serialized and saved using joblib. The MNIST dataset is a collection of 28x28 pixel grayscale images of handwritten digits (0-9), commonly used for training and testing in the field of machine learning. Command Line Arguments: - --batch_size (int): The batch size for the DataLoader. Default is 64. - --download_mnist (flag): Flag to trigger the download of the MNIST dataset. Features: - Downloading the MNIST dataset from the torchvision package. - Applying necessary transformations to the dataset. - Creating a DataLoader to facilitate batch processing during training. - Serializing and saving the DataLoader object for future use. Examples: python script.py --batch_size 64 --download_mnist python script.py --download_mnist Note: - The script uses the 'logging' module for logging information and errors. - The MNIST dataset is stored in './data/raw/' and the DataLoader object in './data/processed/'. - The script is intended to be executed in an environment where Python packages like 'argparse', 'logging', 'joblib', 'torch', and 'torchvision' are installed. Source code in data_loader.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class Loader : \"\"\" This script facilitates the downloading and loading of the MNIST dataset for machine learning tasks. The script uses argparse to accept command-line arguments for batch size and a flag to trigger the download of the MNIST dataset. It defines a `Loader` class responsible for downloading the MNIST dataset and creating a DataLoader object from it. The DataLoader object is then serialized and saved using joblib. The MNIST dataset is a collection of 28x28 pixel grayscale images of handwritten digits (0-9), commonly used for training and testing in the field of machine learning. Command Line Arguments: - --batch_size (int): The batch size for the DataLoader. Default is 64. - --download_mnist (flag): Flag to trigger the download of the MNIST dataset. Features: - Downloading the MNIST dataset from the torchvision package. - Applying necessary transformations to the dataset. - Creating a DataLoader to facilitate batch processing during training. - Serializing and saving the DataLoader object for future use. Examples: python script.py --batch_size 64 --download_mnist python script.py --download_mnist Note: - The script uses the 'logging' module for logging information and errors. - The MNIST dataset is stored in './data/raw/' and the DataLoader object in './data/processed/'. - The script is intended to be executed in an environment where Python packages like 'argparse', 'logging', 'joblib', 'torch', and 'torchvision' are installed. \"\"\" def __init__ ( self , batch_size = 64 ): self . batch_size = batch_size def download_dataset ( self ): logging . info ( \"Downloading dataset...\" . capitalize ()) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 ,), ( 0.5 ,))] ) mnist_data = datasets . MNIST ( root = \"./data/raw/\" , train = True , transform = transforms . ToTensor (), download = True , ) return mnist_data def create_dataloader ( self , mnist_data ): logging . info ( \"Creating dataloader...\" . capitalize ()) dataloader = DataLoader ( mnist_data , batch_size = self . batch_size , shuffle = True ) joblib . dump ( value = dataloader , filename = \"./data/processed/dataloader.pkl\" , )","title":"Dataloader"},{"location":"data_loader/#data_loader.Loader","text":"This script facilitates the downloading and loading of the MNIST dataset for machine learning tasks. The script uses argparse to accept command-line arguments for batch size and a flag to trigger the download of the MNIST dataset. It defines a Loader class responsible for downloading the MNIST dataset and creating a DataLoader object from it. The DataLoader object is then serialized and saved using joblib. The MNIST dataset is a collection of 28x28 pixel grayscale images of handwritten digits (0-9), commonly used for training and testing in the field of machine learning. Command Line Arguments: - --batch_size (int): The batch size for the DataLoader. Default is 64. - --download_mnist (flag): Flag to trigger the download of the MNIST dataset. Features: - Downloading the MNIST dataset from the torchvision package. - Applying necessary transformations to the dataset. - Creating a DataLoader to facilitate batch processing during training. - Serializing and saving the DataLoader object for future use. Examples: python script.py --batch_size 64 --download_mnist python script.py --download_mnist Note: - The script uses the 'logging' module for logging information and errors. - The MNIST dataset is stored in './data/raw/' and the DataLoader object in './data/processed/'. - The script is intended to be executed in an environment where Python packages like 'argparse', 'logging', 'joblib', 'torch', and 'torchvision' are installed. Source code in data_loader.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class Loader : \"\"\" This script facilitates the downloading and loading of the MNIST dataset for machine learning tasks. The script uses argparse to accept command-line arguments for batch size and a flag to trigger the download of the MNIST dataset. It defines a `Loader` class responsible for downloading the MNIST dataset and creating a DataLoader object from it. The DataLoader object is then serialized and saved using joblib. The MNIST dataset is a collection of 28x28 pixel grayscale images of handwritten digits (0-9), commonly used for training and testing in the field of machine learning. Command Line Arguments: - --batch_size (int): The batch size for the DataLoader. Default is 64. - --download_mnist (flag): Flag to trigger the download of the MNIST dataset. Features: - Downloading the MNIST dataset from the torchvision package. - Applying necessary transformations to the dataset. - Creating a DataLoader to facilitate batch processing during training. - Serializing and saving the DataLoader object for future use. Examples: python script.py --batch_size 64 --download_mnist python script.py --download_mnist Note: - The script uses the 'logging' module for logging information and errors. - The MNIST dataset is stored in './data/raw/' and the DataLoader object in './data/processed/'. - The script is intended to be executed in an environment where Python packages like 'argparse', 'logging', 'joblib', 'torch', and 'torchvision' are installed. \"\"\" def __init__ ( self , batch_size = 64 ): self . batch_size = batch_size def download_dataset ( self ): logging . info ( \"Downloading dataset...\" . capitalize ()) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 ,), ( 0.5 ,))] ) mnist_data = datasets . MNIST ( root = \"./data/raw/\" , train = True , transform = transforms . ToTensor (), download = True , ) return mnist_data def create_dataloader ( self , mnist_data ): logging . info ( \"Creating dataloader...\" . capitalize ()) dataloader = DataLoader ( mnist_data , batch_size = self . batch_size , shuffle = True ) joblib . dump ( value = dataloader , filename = \"./data/processed/dataloader.pkl\" , )","title":"Loader"},{"location":"discriminator/","text":"Discriminator Bases: Module A Discriminator class representing a neural network model for distinguishing real images from generated ones. This class inherits from nn.Module and constructs a neural network discriminator model suitable for a Generative Adversarial Network (GAN). The discriminator is designed to take flattened image inputs (such as those from the MNIST dataset) and output a single value indicating the likelihood that the image is real. Attributes: model ( Sequential ) \u2013 A sequential container of layers forming the discriminator network. The architecture is defined based on the layers configuration provided in layers_config . Methods: Name Description forward Defines the forward pass of the discriminator. Parameters: layers_config ( list of tuples ) \u2013 Each tuple in the list contains configuration for a layer in the model, including the number of input features, output features, and the negative slope for the LeakyReLU activation function. The last layer uses a Sigmoid activation function instead of LeakyReLU. Source code in discriminator.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Discriminator ( nn . Module ): \"\"\" A Discriminator class representing a neural network model for distinguishing real images from generated ones. This class inherits from nn.Module and constructs a neural network discriminator model suitable for a Generative Adversarial Network (GAN). The discriminator is designed to take flattened image inputs (such as those from the MNIST dataset) and output a single value indicating the likelihood that the image is real. Attributes: model (torch.nn.Sequential): A sequential container of layers forming the discriminator network. The architecture is defined based on the layers configuration provided in `layers_config`. Methods: forward(x): Defines the forward pass of the discriminator. Parameters: layers_config (list of tuples): Each tuple in the list contains configuration for a layer in the model, including the number of input features, output features, and the negative slope for the LeakyReLU activation function. The last layer uses a Sigmoid activation function instead of LeakyReLU. \"\"\" def __init__ ( self ): super ( Discriminator , self ) . __init__ () layers_config = [ ( 28 * 28 , 512 , 0.2 ), ( 512 , 256 , 0.2 ), ( 256 , 1 ), ] self . model = self . discriminator_block ( layers_config ) def discriminator_block ( self , layers_config ): \"\"\" Builds the discriminator block based on the provided layers configuration. Args: layers_config (list of tuples): Configuration for each layer in the discriminator model. Returns: torch.nn.Sequential: A sequential container of layers forming the discriminator network. \"\"\" layers = OrderedDict () for index , ( input_features , output_features , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \" { index } _layer\" ] = nn . Linear ( in_features = input_features , out_features = output_features ) layers [ f \" { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) # Output layer with Sigmoid activation layers [ \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ \"output_activation\" ] = nn . Sigmoid () return nn . Sequential ( layers ) def forward ( self , x ): \"\"\" Defines the forward pass of the discriminator. Args: x (torch.Tensor): The input tensor containing the image data. Returns: torch.Tensor: The output of the discriminator, representing the probability that the input image is real. \"\"\" if x is not None : x = x . view ( - 1 , 28 * 28 ) x = self . model ( x ) else : x = \"ERROR\" return x discriminator_block ( layers_config ) Builds the discriminator block based on the provided layers configuration. Parameters: layers_config ( list of tuples ) \u2013 Configuration for each layer in the discriminator model. Returns: \u2013 torch.nn.Sequential: A sequential container of layers forming the discriminator network. Source code in discriminator.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def discriminator_block ( self , layers_config ): \"\"\" Builds the discriminator block based on the provided layers configuration. Args: layers_config (list of tuples): Configuration for each layer in the discriminator model. Returns: torch.nn.Sequential: A sequential container of layers forming the discriminator network. \"\"\" layers = OrderedDict () for index , ( input_features , output_features , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \" { index } _layer\" ] = nn . Linear ( in_features = input_features , out_features = output_features ) layers [ f \" { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) # Output layer with Sigmoid activation layers [ \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ \"output_activation\" ] = nn . Sigmoid () return nn . Sequential ( layers ) forward ( x ) Defines the forward pass of the discriminator. Parameters: x ( Tensor ) \u2013 The input tensor containing the image data. Returns: \u2013 torch.Tensor: The output of the discriminator, representing the probability that the input image is real. Source code in discriminator.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def forward ( self , x ): \"\"\" Defines the forward pass of the discriminator. Args: x (torch.Tensor): The input tensor containing the image data. Returns: torch.Tensor: The output of the discriminator, representing the probability that the input image is real. \"\"\" if x is not None : x = x . view ( - 1 , 28 * 28 ) x = self . model ( x ) else : x = \"ERROR\" return x","title":"Discriminator"},{"location":"discriminator/#discriminator.Discriminator","text":"Bases: Module A Discriminator class representing a neural network model for distinguishing real images from generated ones. This class inherits from nn.Module and constructs a neural network discriminator model suitable for a Generative Adversarial Network (GAN). The discriminator is designed to take flattened image inputs (such as those from the MNIST dataset) and output a single value indicating the likelihood that the image is real. Attributes: model ( Sequential ) \u2013 A sequential container of layers forming the discriminator network. The architecture is defined based on the layers configuration provided in layers_config . Methods: Name Description forward Defines the forward pass of the discriminator. Parameters: layers_config ( list of tuples ) \u2013 Each tuple in the list contains configuration for a layer in the model, including the number of input features, output features, and the negative slope for the LeakyReLU activation function. The last layer uses a Sigmoid activation function instead of LeakyReLU. Source code in discriminator.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Discriminator ( nn . Module ): \"\"\" A Discriminator class representing a neural network model for distinguishing real images from generated ones. This class inherits from nn.Module and constructs a neural network discriminator model suitable for a Generative Adversarial Network (GAN). The discriminator is designed to take flattened image inputs (such as those from the MNIST dataset) and output a single value indicating the likelihood that the image is real. Attributes: model (torch.nn.Sequential): A sequential container of layers forming the discriminator network. The architecture is defined based on the layers configuration provided in `layers_config`. Methods: forward(x): Defines the forward pass of the discriminator. Parameters: layers_config (list of tuples): Each tuple in the list contains configuration for a layer in the model, including the number of input features, output features, and the negative slope for the LeakyReLU activation function. The last layer uses a Sigmoid activation function instead of LeakyReLU. \"\"\" def __init__ ( self ): super ( Discriminator , self ) . __init__ () layers_config = [ ( 28 * 28 , 512 , 0.2 ), ( 512 , 256 , 0.2 ), ( 256 , 1 ), ] self . model = self . discriminator_block ( layers_config ) def discriminator_block ( self , layers_config ): \"\"\" Builds the discriminator block based on the provided layers configuration. Args: layers_config (list of tuples): Configuration for each layer in the discriminator model. Returns: torch.nn.Sequential: A sequential container of layers forming the discriminator network. \"\"\" layers = OrderedDict () for index , ( input_features , output_features , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \" { index } _layer\" ] = nn . Linear ( in_features = input_features , out_features = output_features ) layers [ f \" { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) # Output layer with Sigmoid activation layers [ \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ \"output_activation\" ] = nn . Sigmoid () return nn . Sequential ( layers ) def forward ( self , x ): \"\"\" Defines the forward pass of the discriminator. Args: x (torch.Tensor): The input tensor containing the image data. Returns: torch.Tensor: The output of the discriminator, representing the probability that the input image is real. \"\"\" if x is not None : x = x . view ( - 1 , 28 * 28 ) x = self . model ( x ) else : x = \"ERROR\" return x","title":"Discriminator"},{"location":"discriminator/#discriminator.Discriminator.discriminator_block","text":"Builds the discriminator block based on the provided layers configuration. Parameters: layers_config ( list of tuples ) \u2013 Configuration for each layer in the discriminator model. Returns: \u2013 torch.nn.Sequential: A sequential container of layers forming the discriminator network. Source code in discriminator.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def discriminator_block ( self , layers_config ): \"\"\" Builds the discriminator block based on the provided layers configuration. Args: layers_config (list of tuples): Configuration for each layer in the discriminator model. Returns: torch.nn.Sequential: A sequential container of layers forming the discriminator network. \"\"\" layers = OrderedDict () for index , ( input_features , output_features , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \" { index } _layer\" ] = nn . Linear ( in_features = input_features , out_features = output_features ) layers [ f \" { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) # Output layer with Sigmoid activation layers [ \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ \"output_activation\" ] = nn . Sigmoid () return nn . Sequential ( layers )","title":"discriminator_block"},{"location":"discriminator/#discriminator.Discriminator.forward","text":"Defines the forward pass of the discriminator. Parameters: x ( Tensor ) \u2013 The input tensor containing the image data. Returns: \u2013 torch.Tensor: The output of the discriminator, representing the probability that the input image is real. Source code in discriminator.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def forward ( self , x ): \"\"\" Defines the forward pass of the discriminator. Args: x (torch.Tensor): The input tensor containing the image data. Returns: torch.Tensor: The output of the discriminator, representing the probability that the input image is real. \"\"\" if x is not None : x = x . view ( - 1 , 28 * 28 ) x = self . model ( x ) else : x = \"ERROR\" return x","title":"forward"},{"location":"generator/","text":"Generator Bases: Module A generative neural network model for generating images using the DCGAN architecture. Parameters: latent_space ( int , default: 100 ) \u2013 The dimensionality of the latent space noise vector. Default is 100. Attributes: latent_space ( int ) \u2013 The dimensionality of the latent space noise vector. model ( Sequential ) \u2013 The generator model composed of several layers. Example generator = Generator(latent_space=100) noise = torch.randn(64, 100) generated_images = generator(noise) Source code in generator.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class Generator ( nn . Module ): \"\"\" A generative neural network model for generating images using the DCGAN architecture. Args: latent_space (int): The dimensionality of the latent space noise vector. Default is 100. Attributes: latent_space (int): The dimensionality of the latent space noise vector. model (nn.Sequential): The generator model composed of several layers. Example: >>> generator = Generator(latent_space=100) >>> noise = torch.randn(64, 100) >>> generated_images = generator(noise) \"\"\" def __init__ ( self , latent_space = 100 ): \"\"\" Initialize the Generator. Args: latent_space (int, optional): The dimensionality of the latent space noise vector. Default is 100. \"\"\" self . latent_space = latent_space super ( Generator , self ) . __init__ () layers_config = [ ( self . latent_space , 256 , 0.2 ), ( 256 , 512 , 0.2 ), ( 512 , 1024 , 0.2 ), ( 1024 , 28 * 28 ), ] self . model = self . generate_layer ( layers_config = layers_config ) def generate_layer ( self , layers_config ): \"\"\" Create the layers of the generator model based on the provided configuration. Args: layers_config (list): A list of tuples specifying the layer configurations. Returns: nn.Sequential: A sequential model containing the specified layers. Example: >>> layers_config = [(100, 256, 0.02), (256, 512, 0.02), (512, 1024, 0.02), (1024, 28*28)] >>> generator = Generator() >>> generator_model = generator.generate_layer(layers_config) \"\"\" layers = OrderedDict () for index , ( input_feature , out_feature , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \"layer_ { index } \" ] = nn . Linear ( in_features = input_feature , out_features = out_feature ) layers [ f \"layer_ { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) layers [ f \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ f \"output_layer_activation\" ] = nn . Tanh () return nn . Sequential ( layers ) def forward ( self , x ): \"\"\" Forward pass of the generator model. Args: x (torch.Tensor): Input noise tensor sampled from the latent space. Returns: torch.Tensor: Generated images. Example: >>> noise = torch.randn(64, 100) >>> generated_images = generator(noise) \"\"\" if x is not None : x = self . model ( x ) else : x = \"ERROR\" return x . reshape ( - 1 , 1 , 28 , 28 ) __init__ ( latent_space = 100 ) Initialize the Generator. Parameters: latent_space ( int , default: 100 ) \u2013 The dimensionality of the latent space noise vector. Default is 100. Source code in generator.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , latent_space = 100 ): \"\"\" Initialize the Generator. Args: latent_space (int, optional): The dimensionality of the latent space noise vector. Default is 100. \"\"\" self . latent_space = latent_space super ( Generator , self ) . __init__ () layers_config = [ ( self . latent_space , 256 , 0.2 ), ( 256 , 512 , 0.2 ), ( 512 , 1024 , 0.2 ), ( 1024 , 28 * 28 ), ] self . model = self . generate_layer ( layers_config = layers_config ) forward ( x ) Forward pass of the generator model. Parameters: x ( Tensor ) \u2013 Input noise tensor sampled from the latent space. Returns: \u2013 torch.Tensor: Generated images. Example noise = torch.randn(64, 100) generated_images = generator(noise) Source code in generator.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def forward ( self , x ): \"\"\" Forward pass of the generator model. Args: x (torch.Tensor): Input noise tensor sampled from the latent space. Returns: torch.Tensor: Generated images. Example: >>> noise = torch.randn(64, 100) >>> generated_images = generator(noise) \"\"\" if x is not None : x = self . model ( x ) else : x = \"ERROR\" return x . reshape ( - 1 , 1 , 28 , 28 ) generate_layer ( layers_config ) Create the layers of the generator model based on the provided configuration. Parameters: layers_config ( list ) \u2013 A list of tuples specifying the layer configurations. Returns: \u2013 nn.Sequential: A sequential model containing the specified layers. Example layers_config = [(100, 256, 0.02), (256, 512, 0.02), (512, 1024, 0.02), (1024, 28*28)] generator = Generator() generator_model = generator.generate_layer(layers_config) Source code in generator.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def generate_layer ( self , layers_config ): \"\"\" Create the layers of the generator model based on the provided configuration. Args: layers_config (list): A list of tuples specifying the layer configurations. Returns: nn.Sequential: A sequential model containing the specified layers. Example: >>> layers_config = [(100, 256, 0.02), (256, 512, 0.02), (512, 1024, 0.02), (1024, 28*28)] >>> generator = Generator() >>> generator_model = generator.generate_layer(layers_config) \"\"\" layers = OrderedDict () for index , ( input_feature , out_feature , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \"layer_ { index } \" ] = nn . Linear ( in_features = input_feature , out_features = out_feature ) layers [ f \"layer_ { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) layers [ f \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ f \"output_layer_activation\" ] = nn . Tanh () return nn . Sequential ( layers )","title":"Generator"},{"location":"generator/#generator.Generator","text":"Bases: Module A generative neural network model for generating images using the DCGAN architecture. Parameters: latent_space ( int , default: 100 ) \u2013 The dimensionality of the latent space noise vector. Default is 100. Attributes: latent_space ( int ) \u2013 The dimensionality of the latent space noise vector. model ( Sequential ) \u2013 The generator model composed of several layers. Example generator = Generator(latent_space=100) noise = torch.randn(64, 100) generated_images = generator(noise) Source code in generator.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class Generator ( nn . Module ): \"\"\" A generative neural network model for generating images using the DCGAN architecture. Args: latent_space (int): The dimensionality of the latent space noise vector. Default is 100. Attributes: latent_space (int): The dimensionality of the latent space noise vector. model (nn.Sequential): The generator model composed of several layers. Example: >>> generator = Generator(latent_space=100) >>> noise = torch.randn(64, 100) >>> generated_images = generator(noise) \"\"\" def __init__ ( self , latent_space = 100 ): \"\"\" Initialize the Generator. Args: latent_space (int, optional): The dimensionality of the latent space noise vector. Default is 100. \"\"\" self . latent_space = latent_space super ( Generator , self ) . __init__ () layers_config = [ ( self . latent_space , 256 , 0.2 ), ( 256 , 512 , 0.2 ), ( 512 , 1024 , 0.2 ), ( 1024 , 28 * 28 ), ] self . model = self . generate_layer ( layers_config = layers_config ) def generate_layer ( self , layers_config ): \"\"\" Create the layers of the generator model based on the provided configuration. Args: layers_config (list): A list of tuples specifying the layer configurations. Returns: nn.Sequential: A sequential model containing the specified layers. Example: >>> layers_config = [(100, 256, 0.02), (256, 512, 0.02), (512, 1024, 0.02), (1024, 28*28)] >>> generator = Generator() >>> generator_model = generator.generate_layer(layers_config) \"\"\" layers = OrderedDict () for index , ( input_feature , out_feature , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \"layer_ { index } \" ] = nn . Linear ( in_features = input_feature , out_features = out_feature ) layers [ f \"layer_ { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) layers [ f \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ f \"output_layer_activation\" ] = nn . Tanh () return nn . Sequential ( layers ) def forward ( self , x ): \"\"\" Forward pass of the generator model. Args: x (torch.Tensor): Input noise tensor sampled from the latent space. Returns: torch.Tensor: Generated images. Example: >>> noise = torch.randn(64, 100) >>> generated_images = generator(noise) \"\"\" if x is not None : x = self . model ( x ) else : x = \"ERROR\" return x . reshape ( - 1 , 1 , 28 , 28 )","title":"Generator"},{"location":"generator/#generator.Generator.__init__","text":"Initialize the Generator. Parameters: latent_space ( int , default: 100 ) \u2013 The dimensionality of the latent space noise vector. Default is 100. Source code in generator.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , latent_space = 100 ): \"\"\" Initialize the Generator. Args: latent_space (int, optional): The dimensionality of the latent space noise vector. Default is 100. \"\"\" self . latent_space = latent_space super ( Generator , self ) . __init__ () layers_config = [ ( self . latent_space , 256 , 0.2 ), ( 256 , 512 , 0.2 ), ( 512 , 1024 , 0.2 ), ( 1024 , 28 * 28 ), ] self . model = self . generate_layer ( layers_config = layers_config )","title":"__init__"},{"location":"generator/#generator.Generator.forward","text":"Forward pass of the generator model. Parameters: x ( Tensor ) \u2013 Input noise tensor sampled from the latent space. Returns: \u2013 torch.Tensor: Generated images. Example noise = torch.randn(64, 100) generated_images = generator(noise) Source code in generator.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def forward ( self , x ): \"\"\" Forward pass of the generator model. Args: x (torch.Tensor): Input noise tensor sampled from the latent space. Returns: torch.Tensor: Generated images. Example: >>> noise = torch.randn(64, 100) >>> generated_images = generator(noise) \"\"\" if x is not None : x = self . model ( x ) else : x = \"ERROR\" return x . reshape ( - 1 , 1 , 28 , 28 )","title":"forward"},{"location":"generator/#generator.Generator.generate_layer","text":"Create the layers of the generator model based on the provided configuration. Parameters: layers_config ( list ) \u2013 A list of tuples specifying the layer configurations. Returns: \u2013 nn.Sequential: A sequential model containing the specified layers. Example layers_config = [(100, 256, 0.02), (256, 512, 0.02), (512, 1024, 0.02), (1024, 28*28)] generator = Generator() generator_model = generator.generate_layer(layers_config) Source code in generator.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def generate_layer ( self , layers_config ): \"\"\" Create the layers of the generator model based on the provided configuration. Args: layers_config (list): A list of tuples specifying the layer configurations. Returns: nn.Sequential: A sequential model containing the specified layers. Example: >>> layers_config = [(100, 256, 0.02), (256, 512, 0.02), (512, 1024, 0.02), (1024, 28*28)] >>> generator = Generator() >>> generator_model = generator.generate_layer(layers_config) \"\"\" layers = OrderedDict () for index , ( input_feature , out_feature , negative_slope ) in enumerate ( layers_config [: - 1 ] ): layers [ f \"layer_ { index } \" ] = nn . Linear ( in_features = input_feature , out_features = out_feature ) layers [ f \"layer_ { index } _activation\" ] = nn . LeakyReLU ( negative_slope = negative_slope ) layers [ f \"output_layer\" ] = nn . Linear ( in_features = layers_config [ - 1 ][ 0 ], out_features = layers_config [ - 1 ][ 1 ] ) layers [ f \"output_layer_activation\" ] = nn . Tanh () return nn . Sequential ( layers )","title":"generate_layer"},{"location":"trainer/","text":"Trainer A Trainer class for setting up and training a Generative Adversarial Network (GAN). This class is responsible for initializing the GAN components, including the generator and discriminator models, loss functions, optimizers, and the dataloader for input data. It also configures the training environment (device selection) and training parameters. Attributes: - lr (float): Learning rate for the Adam optimizers of both generator and discriminator. - epochs (int): Number of epochs for training the GAN. - betas (tuple): Beta coefficients used for computing running averages of gradient and its square in the Adam optimizer. - latent_space (int): Dimensionality of the latent space for the generator. - dataloader (DataLoader): DataLoader object for loading the training data. - device (torch.device): The computation device (CPU or GPU/MPS) for training. - loss_function (nn.Module): The loss function used during training (Binary Cross Entropy Loss). - generator (nn.Module): The generator model of the GAN. - discriminator (nn.Module): The discriminator model of the GAN. - optimizer_discriminator (optim.Optimizer): The optimizer for the discriminator. - optimizer_generator (optim.Optimizer): The optimizer for the generator. The class assumes the presence of a 'Generator' and 'Discriminator' class for initializing the respective models. The training data is expected to be pre-processed and serialized using joblib, and is loaded into a DataLoader. Usage: To use this class, create an instance and then call its training method (not implemented in this snippet). For example: trainer = Trainer(lr=0.0002, epochs=100, betas=(0.5, 0.999), latent_space=100) trainer.train() # Assuming a 'train' method is implemented Note: - This class requires the 'torch', 'torchvision', 'torch.nn', 'torch.optim', and 'joblib' libraries. - The trainer is designed to work with image data, specifically configured here for the MNIST dataset. - The device selection is automatically determined based on the availability of Apple's Metal Performance Shaders (MPS) for acceleration on compatible hardware. Source code in trainer.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 class Trainer : \"\"\" A Trainer class for setting up and training a Generative Adversarial Network (GAN). This class is responsible for initializing the GAN components, including the generator and discriminator models, loss functions, optimizers, and the dataloader for input data. It also configures the training environment (device selection) and training parameters. Attributes: - lr (float): Learning rate for the Adam optimizers of both generator and discriminator. - epochs (int): Number of epochs for training the GAN. - betas (tuple): Beta coefficients used for computing running averages of gradient and its square in the Adam optimizer. - latent_space (int): Dimensionality of the latent space for the generator. - dataloader (DataLoader): DataLoader object for loading the training data. - device (torch.device): The computation device (CPU or GPU/MPS) for training. - loss_function (nn.Module): The loss function used during training (Binary Cross Entropy Loss). - generator (nn.Module): The generator model of the GAN. - discriminator (nn.Module): The discriminator model of the GAN. - optimizer_discriminator (optim.Optimizer): The optimizer for the discriminator. - optimizer_generator (optim.Optimizer): The optimizer for the generator. The class assumes the presence of a 'Generator' and 'Discriminator' class for initializing the respective models. The training data is expected to be pre-processed and serialized using joblib, and is loaded into a DataLoader. Usage: To use this class, create an instance and then call its training method (not implemented in this snippet). For example: trainer = Trainer(lr=0.0002, epochs=100, betas=(0.5, 0.999), latent_space=100) trainer.train() # Assuming a 'train' method is implemented Note: - This class requires the 'torch', 'torchvision', 'torch.nn', 'torch.optim', and 'joblib' libraries. - The trainer is designed to work with image data, specifically configured here for the MNIST dataset. - The device selection is automatically determined based on the availability of Apple's Metal Performance Shaders (MPS) for acceleration on compatible hardware. \"\"\" def __init__ ( self , lr = 0.0002 , epochs = 100 , betas = ( 0.5 , 0.999 ), latent_space = 100 ): self . lr = lr self . epochs = epochs self . betas = betas self . latent_space = latent_space self . dataloader = joblib . load ( filename = \"./data/processed/dataloader.pkl\" ) self . device = torch . device ( \"mps\" if torch . backends . mps . is_available () else \"cpu\" ) self . loss_function = nn . BCELoss () self . generator = Generator () . to ( self . device ) self . discriminator = Discriminator () . to ( self . device ) self . optimizer_discriminator = optim . Adam ( self . discriminator . parameters (), lr = self . lr , betas = self . betas ) self . optimizer_generator = optim . Adam ( self . generator . parameters (), lr = self . lr , betas = self . betas ) logging . info ( \"Define the discriminator training function\" . capitalize ()) def saved_checkpoint ( self , epoch ): \"\"\" Saves the current state (weights and biases) of the generator model as a checkpoint file. This method is intended to be used during the training process of a Generative Adversarial Network (GAN) to periodically save the state of the generator. The saved state can be used for resuming training, evaluation, or generating data at a later time. The checkpoint is saved in the current directory with a filename that includes the epoch number. Parameters: - epoch (int): The current epoch number in the training process. This is used to tag the saved file for easy identification. The method assumes the presence of a 'generator' attribute in the class instance, which is the neural network model to be saved. It also utilizes Python's logging module to log the success or failure of the save operation. Upon successful saving, a message is logged with the checkpoint file's name. If an exception occurs during saving, an error message is logged with the exception details. Example: - To save a checkpoint of the generator at epoch 5: instance.saved_checkpoint(5) Note: - The method uses 'torch.save' for saving the model state and requires the PyTorch library. - The saved files follow the naming convention 'generator_{epoch}.pth', where {epoch} is replaced with the actual epoch number. - Exception handling is used to manage any errors that might occur during the save operation, with details logged as error messages. \"\"\" logging . info ( \"Save the checkpoint of the discriminator\" . capitalize ()) try : torch . save ( self . generator . state_dict (), \"./models/checkpoints/generator_ {} .pth\" . format ( epoch ), ) except Exception as e : logging . error ( \"Error saving the checkpoint of the generator: {} \" . format ( e )) def train_discriminator ( self , real_samples , fake_samples , real_labels , fake_labels ): \"\"\" Train the discriminator of a Generative Adversarial Network (GAN). This function updates the discriminator by training it on both real and fake samples. It calculates the loss for both real and fake predictions and backpropagates the total loss to update the discriminator's weights. Parameters: - real_samples (Tensor): A batch of real samples from the dataset. - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, representing real samples. - fake_labels (Tensor): A batch of labels, typically zeros, representing fake samples. The function assumes that the discriminator and loss_function are globally accessible, and it also utilizes the optimizer_discriminator for the backpropagation process. Returns: - float: The total loss incurred by the discriminator for the current batch of real and fake samples. \"\"\" self . optimizer_discriminator . zero_grad () real_predicted = self . discriminator ( real_samples ) fake_predicted = self . discriminator ( fake_samples . detach ()) real_predicted_loss = self . loss_function ( real_predicted , real_labels ) fake_predicted_loss = self . loss_function ( fake_predicted , fake_labels ) total_discriminator_loss = real_predicted_loss + fake_predicted_loss total_discriminator_loss . backward () self . optimizer_discriminator . step () return total_discriminator_loss . item () logging . info ( \"Define the generator training function\" . capitalize ()) def train_generator ( self , fake_samples , real_labels ): \"\"\" Train the generator of a Generative Adversarial Network (GAN). This function trains the generator by attempting to fool the discriminator. It updates the generator based on how well it can trick the discriminator into classifying the generated (fake) samples as real. The function calculates the loss by comparing the discriminator's predictions on the fake samples against the 'real' labels and then performs backpropagation to update the generator's weights. Parameters: - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, used as targets for training the generator. This function assumes the availability of a globally accessible discriminator model, a loss function, and the optimizer for the generator (optimizer_generator). Returns: - float: The loss incurred by the generator for the current batch of fake samples, indicating how well the generator is able to fool the discriminator. \"\"\" self . optimizer_generator . zero_grad () fake_predict = self . discriminator ( fake_samples ) generated_loss = self . loss_function ( fake_predict , real_labels ) generated_loss . backward () self . optimizer_generator . step () return generated_loss . item () def train_simple_gan ( self ): \"\"\" Train a Generative Adversarial Network (GAN) consisting of a generator and discriminator. This function orchestrates the training process of a GAN over a specified number of epochs. Training involves alternating between training the discriminator and the generator. The discriminator is trained to distinguish real data from fake data generated by the generator, while the generator is trained to produce data that appears real to the discriminator. Parameters: - epochs (int): The number of training epochs. - latent_space (int): The size of the latent space used to generate noise samples for the generator. - print_interval (int): Interval of steps for printing training progress within each epoch. - dataloader (DataLoader): DataLoader object providing access to the dataset. - device (torch.device): The device (CPU/GPU) on which the training is performed. The function assumes that 'train_discriminator' and 'train_generator' are pre-defined functions that handle the training of the discriminator and generator, respectively. Additionally, 'generator' and 'discriminator' should be predefined models. The function also requires 'np.mean' for calculating average losses, and it prints the training progress at regular intervals. Returns: None: This function does not return a value but prints the training progress and average losses per epoch. \"\"\" print_interval = 100 for epoch in range ( self . epochs ): discriminator_loss = [] generator_loss = [] for i , ( real_samples , _ ) in enumerate ( self . dataloader ): real_samples = real_samples . to ( self . device ) batch_size = real_samples . shape [ 0 ] logging . info ( \"Generate noise samples and fake samples\" . capitalize ()) noise_samples = torch . randn ( batch_size , self . latent_space ) . to ( self . device ) fake_samples = self . generator ( noise_samples ) logging . info ( \"Define labels for real and fake samples\" . capitalize ()) real_labels = torch . ones ( batch_size , 1 ) . to ( self . device ) fake_labels = torch . zeros ( batch_size , 1 ) . to ( self . device ) logging . info ( \"Train the discriminator\" . capitalize ()) d_loss = self . train_discriminator ( real_samples = real_samples , fake_samples = fake_samples , real_labels = real_labels , fake_labels = fake_labels , ) logging . info ( \"Train the generator\" . capitalize ()) g_loss = self . train_generator ( fake_samples = fake_samples , real_labels = real_labels ) discriminator_loss . append ( d_loss ) generator_loss . append ( g_loss ) logging . info ( \"Print training progress every 'print_interval' iterations\" . capitalize () ) if i % print_interval == 0 : print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ], Step [ { i } / { len ( self . dataloader ) } ], d_loss: { d_loss : .4f } , g_loss: { g_loss : .4f } \" ) logging . info ( \"Output average loss at the end of each epoch\" . capitalize ()) print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ] Completed\" ) print ( f \"[==============] Average d_loss: { np . mean ( discriminator_loss ) : .4f } - Average g_loss: { np . mean ( generator_loss ) : .4f } \" ) logging . info ( \"Save checkpoint at the end of each epoch\" . capitalize ()) self . saved_checkpoint ( epoch = epoch + 1 ) saved_checkpoint ( epoch ) Saves the current state (weights and biases) of the generator model as a checkpoint file. This method is intended to be used during the training process of a Generative Adversarial Network (GAN) to periodically save the state of the generator. The saved state can be used for resuming training, evaluation, or generating data at a later time. The checkpoint is saved in the current directory with a filename that includes the epoch number. Parameters: - epoch (int): The current epoch number in the training process. This is used to tag the saved file for easy identification. The method assumes the presence of a 'generator' attribute in the class instance, which is the neural network model to be saved. It also utilizes Python's logging module to log the success or failure of the save operation. Upon successful saving, a message is logged with the checkpoint file's name. If an exception occurs during saving, an error message is logged with the exception details. Example: - To save a checkpoint of the generator at epoch 5: instance.saved_checkpoint(5) Note: - The method uses 'torch.save' for saving the model state and requires the PyTorch library. - The saved files follow the naming convention 'generator_{epoch}.pth', where {epoch} is replaced with the actual epoch number. - Exception handling is used to manage any errors that might occur during the save operation, with details logged as error messages. Source code in trainer.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def saved_checkpoint ( self , epoch ): \"\"\" Saves the current state (weights and biases) of the generator model as a checkpoint file. This method is intended to be used during the training process of a Generative Adversarial Network (GAN) to periodically save the state of the generator. The saved state can be used for resuming training, evaluation, or generating data at a later time. The checkpoint is saved in the current directory with a filename that includes the epoch number. Parameters: - epoch (int): The current epoch number in the training process. This is used to tag the saved file for easy identification. The method assumes the presence of a 'generator' attribute in the class instance, which is the neural network model to be saved. It also utilizes Python's logging module to log the success or failure of the save operation. Upon successful saving, a message is logged with the checkpoint file's name. If an exception occurs during saving, an error message is logged with the exception details. Example: - To save a checkpoint of the generator at epoch 5: instance.saved_checkpoint(5) Note: - The method uses 'torch.save' for saving the model state and requires the PyTorch library. - The saved files follow the naming convention 'generator_{epoch}.pth', where {epoch} is replaced with the actual epoch number. - Exception handling is used to manage any errors that might occur during the save operation, with details logged as error messages. \"\"\" logging . info ( \"Save the checkpoint of the discriminator\" . capitalize ()) try : torch . save ( self . generator . state_dict (), \"./models/checkpoints/generator_ {} .pth\" . format ( epoch ), ) except Exception as e : logging . error ( \"Error saving the checkpoint of the generator: {} \" . format ( e )) train_discriminator ( real_samples , fake_samples , real_labels , fake_labels ) Train the discriminator of a Generative Adversarial Network (GAN). This function updates the discriminator by training it on both real and fake samples. It calculates the loss for both real and fake predictions and backpropagates the total loss to update the discriminator's weights. Parameters: - real_samples (Tensor): A batch of real samples from the dataset. - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, representing real samples. - fake_labels (Tensor): A batch of labels, typically zeros, representing fake samples. The function assumes that the discriminator and loss_function are globally accessible, and it also utilizes the optimizer_discriminator for the backpropagation process. Returns: - float: The total loss incurred by the discriminator for the current batch of real and fake samples. Source code in trainer.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def train_discriminator ( self , real_samples , fake_samples , real_labels , fake_labels ): \"\"\" Train the discriminator of a Generative Adversarial Network (GAN). This function updates the discriminator by training it on both real and fake samples. It calculates the loss for both real and fake predictions and backpropagates the total loss to update the discriminator's weights. Parameters: - real_samples (Tensor): A batch of real samples from the dataset. - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, representing real samples. - fake_labels (Tensor): A batch of labels, typically zeros, representing fake samples. The function assumes that the discriminator and loss_function are globally accessible, and it also utilizes the optimizer_discriminator for the backpropagation process. Returns: - float: The total loss incurred by the discriminator for the current batch of real and fake samples. \"\"\" self . optimizer_discriminator . zero_grad () real_predicted = self . discriminator ( real_samples ) fake_predicted = self . discriminator ( fake_samples . detach ()) real_predicted_loss = self . loss_function ( real_predicted , real_labels ) fake_predicted_loss = self . loss_function ( fake_predicted , fake_labels ) total_discriminator_loss = real_predicted_loss + fake_predicted_loss total_discriminator_loss . backward () self . optimizer_discriminator . step () return total_discriminator_loss . item () train_generator ( fake_samples , real_labels ) Train the generator of a Generative Adversarial Network (GAN). This function trains the generator by attempting to fool the discriminator. It updates the generator based on how well it can trick the discriminator into classifying the generated (fake) samples as real. The function calculates the loss by comparing the discriminator's predictions on the fake samples against the 'real' labels and then performs backpropagation to update the generator's weights. Parameters: - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, used as targets for training the generator. This function assumes the availability of a globally accessible discriminator model, a loss function, and the optimizer for the generator (optimizer_generator). float: The loss incurred by the generator for the current batch of fake samples, indicating how well the generator is able to fool the discriminator. Source code in trainer.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def train_generator ( self , fake_samples , real_labels ): \"\"\" Train the generator of a Generative Adversarial Network (GAN). This function trains the generator by attempting to fool the discriminator. It updates the generator based on how well it can trick the discriminator into classifying the generated (fake) samples as real. The function calculates the loss by comparing the discriminator's predictions on the fake samples against the 'real' labels and then performs backpropagation to update the generator's weights. Parameters: - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, used as targets for training the generator. This function assumes the availability of a globally accessible discriminator model, a loss function, and the optimizer for the generator (optimizer_generator). Returns: - float: The loss incurred by the generator for the current batch of fake samples, indicating how well the generator is able to fool the discriminator. \"\"\" self . optimizer_generator . zero_grad () fake_predict = self . discriminator ( fake_samples ) generated_loss = self . loss_function ( fake_predict , real_labels ) generated_loss . backward () self . optimizer_generator . step () return generated_loss . item () train_simple_gan () Train a Generative Adversarial Network (GAN) consisting of a generator and discriminator. This function orchestrates the training process of a GAN over a specified number of epochs. Training involves alternating between training the discriminator and the generator. The discriminator is trained to distinguish real data from fake data generated by the generator, while the generator is trained to produce data that appears real to the discriminator. Parameters: - epochs (int): The number of training epochs. - latent_space (int): The size of the latent space used to generate noise samples for the generator. - print_interval (int): Interval of steps for printing training progress within each epoch. - dataloader (DataLoader): DataLoader object providing access to the dataset. - device (torch.device): The device (CPU/GPU) on which the training is performed. The function assumes that 'train_discriminator' and 'train_generator' are pre-defined functions that handle the training of the discriminator and generator, respectively. Additionally, 'generator' and 'discriminator' should be predefined models. The function also requires 'np.mean' for calculating average losses, and it prints the training progress at regular intervals. Returns: None: This function does not return a value but prints the training progress and average losses per epoch. Source code in trainer.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def train_simple_gan ( self ): \"\"\" Train a Generative Adversarial Network (GAN) consisting of a generator and discriminator. This function orchestrates the training process of a GAN over a specified number of epochs. Training involves alternating between training the discriminator and the generator. The discriminator is trained to distinguish real data from fake data generated by the generator, while the generator is trained to produce data that appears real to the discriminator. Parameters: - epochs (int): The number of training epochs. - latent_space (int): The size of the latent space used to generate noise samples for the generator. - print_interval (int): Interval of steps for printing training progress within each epoch. - dataloader (DataLoader): DataLoader object providing access to the dataset. - device (torch.device): The device (CPU/GPU) on which the training is performed. The function assumes that 'train_discriminator' and 'train_generator' are pre-defined functions that handle the training of the discriminator and generator, respectively. Additionally, 'generator' and 'discriminator' should be predefined models. The function also requires 'np.mean' for calculating average losses, and it prints the training progress at regular intervals. Returns: None: This function does not return a value but prints the training progress and average losses per epoch. \"\"\" print_interval = 100 for epoch in range ( self . epochs ): discriminator_loss = [] generator_loss = [] for i , ( real_samples , _ ) in enumerate ( self . dataloader ): real_samples = real_samples . to ( self . device ) batch_size = real_samples . shape [ 0 ] logging . info ( \"Generate noise samples and fake samples\" . capitalize ()) noise_samples = torch . randn ( batch_size , self . latent_space ) . to ( self . device ) fake_samples = self . generator ( noise_samples ) logging . info ( \"Define labels for real and fake samples\" . capitalize ()) real_labels = torch . ones ( batch_size , 1 ) . to ( self . device ) fake_labels = torch . zeros ( batch_size , 1 ) . to ( self . device ) logging . info ( \"Train the discriminator\" . capitalize ()) d_loss = self . train_discriminator ( real_samples = real_samples , fake_samples = fake_samples , real_labels = real_labels , fake_labels = fake_labels , ) logging . info ( \"Train the generator\" . capitalize ()) g_loss = self . train_generator ( fake_samples = fake_samples , real_labels = real_labels ) discriminator_loss . append ( d_loss ) generator_loss . append ( g_loss ) logging . info ( \"Print training progress every 'print_interval' iterations\" . capitalize () ) if i % print_interval == 0 : print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ], Step [ { i } / { len ( self . dataloader ) } ], d_loss: { d_loss : .4f } , g_loss: { g_loss : .4f } \" ) logging . info ( \"Output average loss at the end of each epoch\" . capitalize ()) print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ] Completed\" ) print ( f \"[==============] Average d_loss: { np . mean ( discriminator_loss ) : .4f } - Average g_loss: { np . mean ( generator_loss ) : .4f } \" ) logging . info ( \"Save checkpoint at the end of each epoch\" . capitalize ()) self . saved_checkpoint ( epoch = epoch + 1 )","title":"Trainer"},{"location":"trainer/#trainer.Trainer","text":"A Trainer class for setting up and training a Generative Adversarial Network (GAN). This class is responsible for initializing the GAN components, including the generator and discriminator models, loss functions, optimizers, and the dataloader for input data. It also configures the training environment (device selection) and training parameters. Attributes: - lr (float): Learning rate for the Adam optimizers of both generator and discriminator. - epochs (int): Number of epochs for training the GAN. - betas (tuple): Beta coefficients used for computing running averages of gradient and its square in the Adam optimizer. - latent_space (int): Dimensionality of the latent space for the generator. - dataloader (DataLoader): DataLoader object for loading the training data. - device (torch.device): The computation device (CPU or GPU/MPS) for training. - loss_function (nn.Module): The loss function used during training (Binary Cross Entropy Loss). - generator (nn.Module): The generator model of the GAN. - discriminator (nn.Module): The discriminator model of the GAN. - optimizer_discriminator (optim.Optimizer): The optimizer for the discriminator. - optimizer_generator (optim.Optimizer): The optimizer for the generator. The class assumes the presence of a 'Generator' and 'Discriminator' class for initializing the respective models. The training data is expected to be pre-processed and serialized using joblib, and is loaded into a DataLoader. Usage: To use this class, create an instance and then call its training method (not implemented in this snippet). For example: trainer = Trainer(lr=0.0002, epochs=100, betas=(0.5, 0.999), latent_space=100) trainer.train() # Assuming a 'train' method is implemented Note: - This class requires the 'torch', 'torchvision', 'torch.nn', 'torch.optim', and 'joblib' libraries. - The trainer is designed to work with image data, specifically configured here for the MNIST dataset. - The device selection is automatically determined based on the availability of Apple's Metal Performance Shaders (MPS) for acceleration on compatible hardware. Source code in trainer.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 class Trainer : \"\"\" A Trainer class for setting up and training a Generative Adversarial Network (GAN). This class is responsible for initializing the GAN components, including the generator and discriminator models, loss functions, optimizers, and the dataloader for input data. It also configures the training environment (device selection) and training parameters. Attributes: - lr (float): Learning rate for the Adam optimizers of both generator and discriminator. - epochs (int): Number of epochs for training the GAN. - betas (tuple): Beta coefficients used for computing running averages of gradient and its square in the Adam optimizer. - latent_space (int): Dimensionality of the latent space for the generator. - dataloader (DataLoader): DataLoader object for loading the training data. - device (torch.device): The computation device (CPU or GPU/MPS) for training. - loss_function (nn.Module): The loss function used during training (Binary Cross Entropy Loss). - generator (nn.Module): The generator model of the GAN. - discriminator (nn.Module): The discriminator model of the GAN. - optimizer_discriminator (optim.Optimizer): The optimizer for the discriminator. - optimizer_generator (optim.Optimizer): The optimizer for the generator. The class assumes the presence of a 'Generator' and 'Discriminator' class for initializing the respective models. The training data is expected to be pre-processed and serialized using joblib, and is loaded into a DataLoader. Usage: To use this class, create an instance and then call its training method (not implemented in this snippet). For example: trainer = Trainer(lr=0.0002, epochs=100, betas=(0.5, 0.999), latent_space=100) trainer.train() # Assuming a 'train' method is implemented Note: - This class requires the 'torch', 'torchvision', 'torch.nn', 'torch.optim', and 'joblib' libraries. - The trainer is designed to work with image data, specifically configured here for the MNIST dataset. - The device selection is automatically determined based on the availability of Apple's Metal Performance Shaders (MPS) for acceleration on compatible hardware. \"\"\" def __init__ ( self , lr = 0.0002 , epochs = 100 , betas = ( 0.5 , 0.999 ), latent_space = 100 ): self . lr = lr self . epochs = epochs self . betas = betas self . latent_space = latent_space self . dataloader = joblib . load ( filename = \"./data/processed/dataloader.pkl\" ) self . device = torch . device ( \"mps\" if torch . backends . mps . is_available () else \"cpu\" ) self . loss_function = nn . BCELoss () self . generator = Generator () . to ( self . device ) self . discriminator = Discriminator () . to ( self . device ) self . optimizer_discriminator = optim . Adam ( self . discriminator . parameters (), lr = self . lr , betas = self . betas ) self . optimizer_generator = optim . Adam ( self . generator . parameters (), lr = self . lr , betas = self . betas ) logging . info ( \"Define the discriminator training function\" . capitalize ()) def saved_checkpoint ( self , epoch ): \"\"\" Saves the current state (weights and biases) of the generator model as a checkpoint file. This method is intended to be used during the training process of a Generative Adversarial Network (GAN) to periodically save the state of the generator. The saved state can be used for resuming training, evaluation, or generating data at a later time. The checkpoint is saved in the current directory with a filename that includes the epoch number. Parameters: - epoch (int): The current epoch number in the training process. This is used to tag the saved file for easy identification. The method assumes the presence of a 'generator' attribute in the class instance, which is the neural network model to be saved. It also utilizes Python's logging module to log the success or failure of the save operation. Upon successful saving, a message is logged with the checkpoint file's name. If an exception occurs during saving, an error message is logged with the exception details. Example: - To save a checkpoint of the generator at epoch 5: instance.saved_checkpoint(5) Note: - The method uses 'torch.save' for saving the model state and requires the PyTorch library. - The saved files follow the naming convention 'generator_{epoch}.pth', where {epoch} is replaced with the actual epoch number. - Exception handling is used to manage any errors that might occur during the save operation, with details logged as error messages. \"\"\" logging . info ( \"Save the checkpoint of the discriminator\" . capitalize ()) try : torch . save ( self . generator . state_dict (), \"./models/checkpoints/generator_ {} .pth\" . format ( epoch ), ) except Exception as e : logging . error ( \"Error saving the checkpoint of the generator: {} \" . format ( e )) def train_discriminator ( self , real_samples , fake_samples , real_labels , fake_labels ): \"\"\" Train the discriminator of a Generative Adversarial Network (GAN). This function updates the discriminator by training it on both real and fake samples. It calculates the loss for both real and fake predictions and backpropagates the total loss to update the discriminator's weights. Parameters: - real_samples (Tensor): A batch of real samples from the dataset. - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, representing real samples. - fake_labels (Tensor): A batch of labels, typically zeros, representing fake samples. The function assumes that the discriminator and loss_function are globally accessible, and it also utilizes the optimizer_discriminator for the backpropagation process. Returns: - float: The total loss incurred by the discriminator for the current batch of real and fake samples. \"\"\" self . optimizer_discriminator . zero_grad () real_predicted = self . discriminator ( real_samples ) fake_predicted = self . discriminator ( fake_samples . detach ()) real_predicted_loss = self . loss_function ( real_predicted , real_labels ) fake_predicted_loss = self . loss_function ( fake_predicted , fake_labels ) total_discriminator_loss = real_predicted_loss + fake_predicted_loss total_discriminator_loss . backward () self . optimizer_discriminator . step () return total_discriminator_loss . item () logging . info ( \"Define the generator training function\" . capitalize ()) def train_generator ( self , fake_samples , real_labels ): \"\"\" Train the generator of a Generative Adversarial Network (GAN). This function trains the generator by attempting to fool the discriminator. It updates the generator based on how well it can trick the discriminator into classifying the generated (fake) samples as real. The function calculates the loss by comparing the discriminator's predictions on the fake samples against the 'real' labels and then performs backpropagation to update the generator's weights. Parameters: - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, used as targets for training the generator. This function assumes the availability of a globally accessible discriminator model, a loss function, and the optimizer for the generator (optimizer_generator). Returns: - float: The loss incurred by the generator for the current batch of fake samples, indicating how well the generator is able to fool the discriminator. \"\"\" self . optimizer_generator . zero_grad () fake_predict = self . discriminator ( fake_samples ) generated_loss = self . loss_function ( fake_predict , real_labels ) generated_loss . backward () self . optimizer_generator . step () return generated_loss . item () def train_simple_gan ( self ): \"\"\" Train a Generative Adversarial Network (GAN) consisting of a generator and discriminator. This function orchestrates the training process of a GAN over a specified number of epochs. Training involves alternating between training the discriminator and the generator. The discriminator is trained to distinguish real data from fake data generated by the generator, while the generator is trained to produce data that appears real to the discriminator. Parameters: - epochs (int): The number of training epochs. - latent_space (int): The size of the latent space used to generate noise samples for the generator. - print_interval (int): Interval of steps for printing training progress within each epoch. - dataloader (DataLoader): DataLoader object providing access to the dataset. - device (torch.device): The device (CPU/GPU) on which the training is performed. The function assumes that 'train_discriminator' and 'train_generator' are pre-defined functions that handle the training of the discriminator and generator, respectively. Additionally, 'generator' and 'discriminator' should be predefined models. The function also requires 'np.mean' for calculating average losses, and it prints the training progress at regular intervals. Returns: None: This function does not return a value but prints the training progress and average losses per epoch. \"\"\" print_interval = 100 for epoch in range ( self . epochs ): discriminator_loss = [] generator_loss = [] for i , ( real_samples , _ ) in enumerate ( self . dataloader ): real_samples = real_samples . to ( self . device ) batch_size = real_samples . shape [ 0 ] logging . info ( \"Generate noise samples and fake samples\" . capitalize ()) noise_samples = torch . randn ( batch_size , self . latent_space ) . to ( self . device ) fake_samples = self . generator ( noise_samples ) logging . info ( \"Define labels for real and fake samples\" . capitalize ()) real_labels = torch . ones ( batch_size , 1 ) . to ( self . device ) fake_labels = torch . zeros ( batch_size , 1 ) . to ( self . device ) logging . info ( \"Train the discriminator\" . capitalize ()) d_loss = self . train_discriminator ( real_samples = real_samples , fake_samples = fake_samples , real_labels = real_labels , fake_labels = fake_labels , ) logging . info ( \"Train the generator\" . capitalize ()) g_loss = self . train_generator ( fake_samples = fake_samples , real_labels = real_labels ) discriminator_loss . append ( d_loss ) generator_loss . append ( g_loss ) logging . info ( \"Print training progress every 'print_interval' iterations\" . capitalize () ) if i % print_interval == 0 : print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ], Step [ { i } / { len ( self . dataloader ) } ], d_loss: { d_loss : .4f } , g_loss: { g_loss : .4f } \" ) logging . info ( \"Output average loss at the end of each epoch\" . capitalize ()) print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ] Completed\" ) print ( f \"[==============] Average d_loss: { np . mean ( discriminator_loss ) : .4f } - Average g_loss: { np . mean ( generator_loss ) : .4f } \" ) logging . info ( \"Save checkpoint at the end of each epoch\" . capitalize ()) self . saved_checkpoint ( epoch = epoch + 1 )","title":"Trainer"},{"location":"trainer/#trainer.Trainer.saved_checkpoint","text":"Saves the current state (weights and biases) of the generator model as a checkpoint file. This method is intended to be used during the training process of a Generative Adversarial Network (GAN) to periodically save the state of the generator. The saved state can be used for resuming training, evaluation, or generating data at a later time. The checkpoint is saved in the current directory with a filename that includes the epoch number. Parameters: - epoch (int): The current epoch number in the training process. This is used to tag the saved file for easy identification. The method assumes the presence of a 'generator' attribute in the class instance, which is the neural network model to be saved. It also utilizes Python's logging module to log the success or failure of the save operation. Upon successful saving, a message is logged with the checkpoint file's name. If an exception occurs during saving, an error message is logged with the exception details. Example: - To save a checkpoint of the generator at epoch 5: instance.saved_checkpoint(5) Note: - The method uses 'torch.save' for saving the model state and requires the PyTorch library. - The saved files follow the naming convention 'generator_{epoch}.pth', where {epoch} is replaced with the actual epoch number. - Exception handling is used to manage any errors that might occur during the save operation, with details logged as error messages. Source code in trainer.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def saved_checkpoint ( self , epoch ): \"\"\" Saves the current state (weights and biases) of the generator model as a checkpoint file. This method is intended to be used during the training process of a Generative Adversarial Network (GAN) to periodically save the state of the generator. The saved state can be used for resuming training, evaluation, or generating data at a later time. The checkpoint is saved in the current directory with a filename that includes the epoch number. Parameters: - epoch (int): The current epoch number in the training process. This is used to tag the saved file for easy identification. The method assumes the presence of a 'generator' attribute in the class instance, which is the neural network model to be saved. It also utilizes Python's logging module to log the success or failure of the save operation. Upon successful saving, a message is logged with the checkpoint file's name. If an exception occurs during saving, an error message is logged with the exception details. Example: - To save a checkpoint of the generator at epoch 5: instance.saved_checkpoint(5) Note: - The method uses 'torch.save' for saving the model state and requires the PyTorch library. - The saved files follow the naming convention 'generator_{epoch}.pth', where {epoch} is replaced with the actual epoch number. - Exception handling is used to manage any errors that might occur during the save operation, with details logged as error messages. \"\"\" logging . info ( \"Save the checkpoint of the discriminator\" . capitalize ()) try : torch . save ( self . generator . state_dict (), \"./models/checkpoints/generator_ {} .pth\" . format ( epoch ), ) except Exception as e : logging . error ( \"Error saving the checkpoint of the generator: {} \" . format ( e ))","title":"saved_checkpoint"},{"location":"trainer/#trainer.Trainer.train_discriminator","text":"Train the discriminator of a Generative Adversarial Network (GAN). This function updates the discriminator by training it on both real and fake samples. It calculates the loss for both real and fake predictions and backpropagates the total loss to update the discriminator's weights. Parameters: - real_samples (Tensor): A batch of real samples from the dataset. - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, representing real samples. - fake_labels (Tensor): A batch of labels, typically zeros, representing fake samples. The function assumes that the discriminator and loss_function are globally accessible, and it also utilizes the optimizer_discriminator for the backpropagation process. Returns: - float: The total loss incurred by the discriminator for the current batch of real and fake samples. Source code in trainer.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def train_discriminator ( self , real_samples , fake_samples , real_labels , fake_labels ): \"\"\" Train the discriminator of a Generative Adversarial Network (GAN). This function updates the discriminator by training it on both real and fake samples. It calculates the loss for both real and fake predictions and backpropagates the total loss to update the discriminator's weights. Parameters: - real_samples (Tensor): A batch of real samples from the dataset. - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, representing real samples. - fake_labels (Tensor): A batch of labels, typically zeros, representing fake samples. The function assumes that the discriminator and loss_function are globally accessible, and it also utilizes the optimizer_discriminator for the backpropagation process. Returns: - float: The total loss incurred by the discriminator for the current batch of real and fake samples. \"\"\" self . optimizer_discriminator . zero_grad () real_predicted = self . discriminator ( real_samples ) fake_predicted = self . discriminator ( fake_samples . detach ()) real_predicted_loss = self . loss_function ( real_predicted , real_labels ) fake_predicted_loss = self . loss_function ( fake_predicted , fake_labels ) total_discriminator_loss = real_predicted_loss + fake_predicted_loss total_discriminator_loss . backward () self . optimizer_discriminator . step () return total_discriminator_loss . item ()","title":"train_discriminator"},{"location":"trainer/#trainer.Trainer.train_generator","text":"Train the generator of a Generative Adversarial Network (GAN). This function trains the generator by attempting to fool the discriminator. It updates the generator based on how well it can trick the discriminator into classifying the generated (fake) samples as real. The function calculates the loss by comparing the discriminator's predictions on the fake samples against the 'real' labels and then performs backpropagation to update the generator's weights. Parameters: - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, used as targets for training the generator. This function assumes the availability of a globally accessible discriminator model, a loss function, and the optimizer for the generator (optimizer_generator). float: The loss incurred by the generator for the current batch of fake samples, indicating how well the generator is able to fool the discriminator. Source code in trainer.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def train_generator ( self , fake_samples , real_labels ): \"\"\" Train the generator of a Generative Adversarial Network (GAN). This function trains the generator by attempting to fool the discriminator. It updates the generator based on how well it can trick the discriminator into classifying the generated (fake) samples as real. The function calculates the loss by comparing the discriminator's predictions on the fake samples against the 'real' labels and then performs backpropagation to update the generator's weights. Parameters: - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator. - real_labels (Tensor): A batch of labels, typically ones, used as targets for training the generator. This function assumes the availability of a globally accessible discriminator model, a loss function, and the optimizer for the generator (optimizer_generator). Returns: - float: The loss incurred by the generator for the current batch of fake samples, indicating how well the generator is able to fool the discriminator. \"\"\" self . optimizer_generator . zero_grad () fake_predict = self . discriminator ( fake_samples ) generated_loss = self . loss_function ( fake_predict , real_labels ) generated_loss . backward () self . optimizer_generator . step () return generated_loss . item ()","title":"train_generator"},{"location":"trainer/#trainer.Trainer.train_simple_gan","text":"Train a Generative Adversarial Network (GAN) consisting of a generator and discriminator. This function orchestrates the training process of a GAN over a specified number of epochs. Training involves alternating between training the discriminator and the generator. The discriminator is trained to distinguish real data from fake data generated by the generator, while the generator is trained to produce data that appears real to the discriminator. Parameters: - epochs (int): The number of training epochs. - latent_space (int): The size of the latent space used to generate noise samples for the generator. - print_interval (int): Interval of steps for printing training progress within each epoch. - dataloader (DataLoader): DataLoader object providing access to the dataset. - device (torch.device): The device (CPU/GPU) on which the training is performed. The function assumes that 'train_discriminator' and 'train_generator' are pre-defined functions that handle the training of the discriminator and generator, respectively. Additionally, 'generator' and 'discriminator' should be predefined models. The function also requires 'np.mean' for calculating average losses, and it prints the training progress at regular intervals. Returns: None: This function does not return a value but prints the training progress and average losses per epoch. Source code in trainer.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def train_simple_gan ( self ): \"\"\" Train a Generative Adversarial Network (GAN) consisting of a generator and discriminator. This function orchestrates the training process of a GAN over a specified number of epochs. Training involves alternating between training the discriminator and the generator. The discriminator is trained to distinguish real data from fake data generated by the generator, while the generator is trained to produce data that appears real to the discriminator. Parameters: - epochs (int): The number of training epochs. - latent_space (int): The size of the latent space used to generate noise samples for the generator. - print_interval (int): Interval of steps for printing training progress within each epoch. - dataloader (DataLoader): DataLoader object providing access to the dataset. - device (torch.device): The device (CPU/GPU) on which the training is performed. The function assumes that 'train_discriminator' and 'train_generator' are pre-defined functions that handle the training of the discriminator and generator, respectively. Additionally, 'generator' and 'discriminator' should be predefined models. The function also requires 'np.mean' for calculating average losses, and it prints the training progress at regular intervals. Returns: None: This function does not return a value but prints the training progress and average losses per epoch. \"\"\" print_interval = 100 for epoch in range ( self . epochs ): discriminator_loss = [] generator_loss = [] for i , ( real_samples , _ ) in enumerate ( self . dataloader ): real_samples = real_samples . to ( self . device ) batch_size = real_samples . shape [ 0 ] logging . info ( \"Generate noise samples and fake samples\" . capitalize ()) noise_samples = torch . randn ( batch_size , self . latent_space ) . to ( self . device ) fake_samples = self . generator ( noise_samples ) logging . info ( \"Define labels for real and fake samples\" . capitalize ()) real_labels = torch . ones ( batch_size , 1 ) . to ( self . device ) fake_labels = torch . zeros ( batch_size , 1 ) . to ( self . device ) logging . info ( \"Train the discriminator\" . capitalize ()) d_loss = self . train_discriminator ( real_samples = real_samples , fake_samples = fake_samples , real_labels = real_labels , fake_labels = fake_labels , ) logging . info ( \"Train the generator\" . capitalize ()) g_loss = self . train_generator ( fake_samples = fake_samples , real_labels = real_labels ) discriminator_loss . append ( d_loss ) generator_loss . append ( g_loss ) logging . info ( \"Print training progress every 'print_interval' iterations\" . capitalize () ) if i % print_interval == 0 : print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ], Step [ { i } / { len ( self . dataloader ) } ], d_loss: { d_loss : .4f } , g_loss: { g_loss : .4f } \" ) logging . info ( \"Output average loss at the end of each epoch\" . capitalize ()) print ( f \"Epoch [ { epoch + 1 } / { self . epochs } ] Completed\" ) print ( f \"[==============] Average d_loss: { np . mean ( discriminator_loss ) : .4f } - Average g_loss: { np . mean ( generator_loss ) : .4f } \" ) logging . info ( \"Save checkpoint at the end of each epoch\" . capitalize ()) self . saved_checkpoint ( epoch = epoch + 1 )","title":"train_simple_gan"}]}